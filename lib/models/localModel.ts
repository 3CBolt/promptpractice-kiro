import { ModelResult } from '@/types';

/**
 * Local model implementation with deterministic stub responses
 * Provides offline functionality without requiring API keys
 */

interface LocalModelResponse {
  text: string;
  latencyMs: number;
  usageTokens: number;
}

// Deterministic responses based on prompt content
const STUB_RESPONSES: Record<string, string> = {
  // Default responses for common prompt patterns
  default: "This is a sample response from the local model. In a real implementation, this would be generated by an AI model. The response demonstrates basic functionality while the app runs offline.",
  
  // Responses for different prompt types
  question: "Based on your question, here's a thoughtful response that addresses the key points you've raised. This demonstrates how the model would typically respond to inquiry-based prompts.",
  
  creative: "Here's a creative response that showcases imaginative thinking and original ideas. This type of response would typically be generated for creative writing or brainstorming prompts.",
  
  analytical: "This response provides a structured analysis of the topic, breaking down complex concepts into understandable components. It demonstrates logical reasoning and systematic thinking.",
  
  instructional: "Here are step-by-step instructions that clearly explain the process. This response format is ideal for how-to questions and procedural guidance.",
  
  conversational: "This is a friendly, conversational response that maintains an engaging tone while providing helpful information. It demonstrates natural language interaction."
};

/**
 * Determines response type based on prompt content
 */
function getResponseType(prompt: string): string {
  const lowerPrompt = prompt.toLowerCase();
  
  if (lowerPrompt.includes('?') || lowerPrompt.includes('what') || lowerPrompt.includes('how') || lowerPrompt.includes('why')) {
    return 'question';
  }
  if (lowerPrompt.includes('create') || lowerPrompt.includes('write') || lowerPrompt.includes('imagine')) {
    return 'creative';
  }
  if (lowerPrompt.includes('analyze') || lowerPrompt.includes('compare') || lowerPrompt.includes('evaluate')) {
    return 'analytical';
  }
  if (lowerPrompt.includes('explain') || lowerPrompt.includes('teach') || lowerPrompt.includes('show')) {
    return 'instructional';
  }
  if (lowerPrompt.includes('chat') || lowerPrompt.includes('talk') || lowerPrompt.includes('discuss')) {
    return 'conversational';
  }
  
  return 'default';
}

/**
 * Simulates realistic latency based on prompt length
 */
function calculateLatency(prompt: string): number {
  const baseLatency = 200; // Base 200ms
  const lengthFactor = Math.min(prompt.length * 2, 800); // Up to 800ms additional
  const randomVariation = Math.random() * 100; // 0-100ms random variation
  
  return Math.round(baseLatency + lengthFactor + randomVariation);
}

/**
 * Estimates token usage based on response length
 */
function estimateTokens(text: string): number {
  // Rough estimation: ~4 characters per token
  return Math.round(text.length / 4);
}

/**
 * Calls the local model with deterministic responses
 */
export async function callLocalModel(
  modelId: string,
  prompt: string,
  systemPrompt?: string
): Promise<LocalModelResponse> {
  // Simulate processing time
  const latencyMs = calculateLatency(prompt);
  await new Promise(resolve => setTimeout(resolve, latencyMs));
  
  // Determine response type and get appropriate response
  const responseType = getResponseType(prompt);
  let baseResponse = STUB_RESPONSES[responseType] || STUB_RESPONSES.default;
  
  // Add system prompt influence if provided
  if (systemPrompt) {
    baseResponse = `Following the system guidance: ${baseResponse}`;
  }
  
  // Add model-specific variation
  const modelVariations: Record<string, string> = {
    'local-stub': baseResponse,
    'local-creative': `${baseResponse} This response emphasizes creative and imaginative elements.`,
    'local-analytical': `${baseResponse} This response focuses on logical analysis and structured thinking.`
  };
  
  const text = modelVariations[modelId] || baseResponse;
  const usageTokens = estimateTokens(text);
  
  return {
    text,
    latencyMs,
    usageTokens
  };
}

/**
 * Converts local model response to ModelResult format
 */
export async function getLocalModelResult(
  modelId: string,
  prompt: string,
  systemPrompt?: string
): Promise<ModelResult> {
  const response = await callLocalModel(modelId, prompt, systemPrompt);
  
  return {
    modelId,
    text: response.text,
    latencyMs: response.latencyMs,
    usageTokens: response.usageTokens,
    source: 'sample' // Local models are marked as sample data
  };
}